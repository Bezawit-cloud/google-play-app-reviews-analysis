# Customer Experience Analytics for Fintech Apps â€” Task 1  
### 10 Academy: Artificial Intelligence Mastery â€” Week 2 Challenge

---

## ðŸ“Œ Project Overview  
This project analyzes customer satisfaction with mobile banking apps by scraping and preprocessing Google Play Store reviews for three major Ethiopian banks:

- **Commercial Bank of Ethiopia (CBE)**
- **Bank of Abyssinia (BOA)**
- **Dashen Bank**

Task 1 focuses on **data collection** and **preprocessing** to prepare a clean dataset for NLP analysis and visualization in later tasks.

---

## ðŸŽ¯ Task 1 Objectives  

- Scrape user reviews from the Google Play Store using `google-play-scraper`.
- Collect **400+ reviews per bank** (1,200 total recommended).
- Clean and preprocess the data:
  - Remove duplicates  
  - Handle missing values  
  - Standardize date format  
- Save the cleaned dataset as a CSV (kept locally, not uploaded to GitHub).
- Maintain a clean GitHub repository with `.gitignore`, `requirements.txt`, and a clear folder structure.

google-play-app-reviews-analysis/
â”‚
â”œâ”€â”€ data/ # (Hidden from GitHub using .gitignore)
â”‚ â”œâ”€â”€ raw/ # Raw scraped review CSVs
â”‚ â””â”€â”€ processed/ # Cleaned final dataset
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ scraping.ipynb # Jupyter notebook for scraping + preprocessing
â”‚
â”œâ”€â”€ src/
â”‚ â””â”€â”€ scraping.py # (Optional) Python script version of scraping
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ðŸ›  Tools & Libraries  

- Python 3.10+
- `google-play-scraper`
- `pandas`
- `numpy`
- Jupyter Notebook
- Git & GitHub

Install dependencies:

```bash
pip install -r requirements.txt

ðŸ“˜ Scraping Methodology
âœ” App IDs Used
Bank	App ID
CBE	com.combanketh.mobilebanking
BOA	com.boa.boaMobileBanking
Dashen	com.dashen.dashensuperapp
âœ” Scraping Function
def scrape_reviews(app_id, bank_name, n_reviews=400):
    reviews_list = []

    while len(reviews_list) < n_reviews:
        data, _ = reviews(
            app_id,
            lang="en",
            country="et",
            sort=Sort.NEWEST,
            count=200,
            filter_score_with=None
        )
        reviews_list.extend(data)

    df = pd.DataFrame(data)[["content", "score", "at"]]
    df.columns = ["review", "rating", "date"]
    df["bank"] = bank_name
    df["source"] = "Google Play"

    return df

ðŸ§¹ Preprocessing Steps

Converted review date â†’ YYYY-MM-DD

Removed duplicate reviews

Removed rows with missing review text

Combined all banks into one dataframe

Saved final cleaned CSV to:

data/processed/ethiopian_banks_reviews.csv


(This folder is hidden in Git using .gitignore.)

ðŸ“Š Output Summary

Total Reviews Scraped: ~500+ after cleaning

Final Columns:

review

rating

date

bank

source

Dataset is now prepared for sentiment analysis and theme extraction in Task 2.

ðŸ”’ Why the Data Folder Is Hidden

Large data files should not be pushed to GitHub.
.gitignore contains:

data/
*.csv


This keeps the repository clean and professional.

ðŸŒ¿ Git Branch Workflow

Branches used:

main â†’ stable code

task-1 â†’ development branch for Task 1

Example commit messages:

"Add scraping function for CBE, BOA, Dashen"

"Clean dataset and normalize dates"

"Hide data folder using .gitignore"

"Add Task 1 README"

## ðŸ“‚ Project Structure  

